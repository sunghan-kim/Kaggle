{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the \"Topic\" of toxicity\n",
    "\n",
    "[https://www.kaggle.com/jagangupta/understanding-the-topic-of-toxicity](https://www.kaggle.com/jagangupta/understanding-the-topic-of-toxicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is an extension of the EDA notebook: Stop the S@#$ - [Toxic Comments EDA](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Topic Modeling:\n",
    "\n",
    "Topic modeling can be a useful tool to summarize the context of a huge corpus(text) by guessing what the \"Topic\" or the general.\n",
    "\n",
    "This can also be used as inputs to our classifier if they can identify patterns or \"Topics\" that indicate toxicity.\n",
    "\n",
    "Let's find out!\n",
    "\n",
    "The steps followed in this kernel:\n",
    "\n",
    "- Preprocessing (Tokenization using gensim's simple_preprocess)\n",
    "- Cleaning\n",
    "  - Stop word removal\n",
    "  - Bigram collation\n",
    "  - Lemmatization\n",
    "- Creation of dictionary (list of all words in the cleaned text)\n",
    "- Topic modeling using LDA\n",
    "- Visualization with pyLDAviz\n",
    "- Convert topics to sparse vectors\n",
    "- Feed sparse vectors to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shkim\\AppData\\Local\\conda\\conda\\envs\\python36\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "\n",
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# nlp\n",
    "import string\n",
    "import re # for regex\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Modeling\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "# settings\n",
    "start_time = time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "# constants\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till import: 2.7765743732452393 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# importing the dataset\n",
    "train = pd.read_csv(\"../../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "test = pd.read_csv(\"../../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "\n",
    "end_import = time.time()\n",
    "\n",
    "print(\"Time till import:\", end_import - start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to seperate sentenses into words\n",
    "def preprocess(comment):\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from input comment\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(comment, deacc=True, min_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the comments\n",
    "train_text = train.comment_text.apply(lambda x: preprocess(x))\n",
    "test_text = test.comment_text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = train_text.append(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments: 312735\n",
      "Before preprocessing: How could I post before the block expires?  The funny thing is, you think I'm being uncivil!\n",
      "After preprocessing: ['how', 'could', 'post', 'before', 'the', 'block', 'expires', 'the', 'funny', 'thing', 'you', 'think', 'being', 'uncivil']\n"
     ]
    }
   ],
   "source": [
    "# checks\n",
    "print(\"Total number of comments:\", len(all_text))\n",
    "print(\"Before preprocessing:\", train.comment_text.iloc[30])\n",
    "print(\"After preprocessing:\", all_text.iloc[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrases help us group together bigrams : new + york --> new_york\n",
    "bigram = gensim.models.Phrases(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Pharses: ['how', 'could', 'post', 'before', 'the', 'block', 'expires', 'the', 'funny', 'thing', 'you', 'think', 'being', 'uncivil']\n",
      "After Pharses: ['how', 'could', 'post', 'before', 'the', 'block_expires', 'the', 'funny_thing', 'you', 'think', 'being_uncivil']\n"
     ]
    }
   ],
   "source": [
    "# check bigram collation functionality\n",
    "print(\"Before Pharses:\", all_text.iloc[30])\n",
    "print(\"After Pharses:\", bigram[all_text.iloc[30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['block_expires', 'funny_thing', 'being_uncivil']\n"
     ]
    }
   ],
   "source": [
    "diff = []\n",
    "\n",
    "for word in bigram[all_text.iloc[30]]:\n",
    "    if word not in all_text.iloc[30]:\n",
    "        diff.append(word)\n",
    "        \n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(word_list):\n",
    "    \"\"\"\n",
    "    Function to clean the pre-processed word lists\n",
    "    \n",
    "    Following transformations will be done\n",
    "    1) Stop words removal from the nltk stopword list\n",
    "    2) Bigram collation \n",
    "       (Finding common bigrams and grouping them together using gensim.models.phrases)\n",
    "    3) Lemmatization\n",
    "       (Converting word to its root form : babies --> baby ; children --> child)\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove stop words\n",
    "    clean_words = [w for w in word_list if not w in eng_stopwords]\n",
    "    \n",
    "    # collect bigrams\n",
    "    clean_words = bigram[clean_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    clean_words = [lem.lemmatize(word, \"v\") for word in clean_words]\n",
    "    \n",
    "    return (clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean: ['aww', 'matches', 'this', 'background', 'colour', 'seemingly', 'stuck', 'with', 'thanks', 'talk', 'january', 'utc']\n",
      "After clean: ['aww', 'match', 'background', 'colour', 'seemingly', 'stick', 'thank', 'talk', 'january_utc']\n"
     ]
    }
   ],
   "source": [
    "# check clean function\n",
    "print(\"Before clean:\", all_text.iloc[1])\n",
    "print(\"After clean:\", clean(all_text.iloc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till cleaning corpus: 437.84178829193115 s\n"
     ]
    }
   ],
   "source": [
    "# scale it to all text\n",
    "all_text = all_text.apply(lambda x: clean(x))\n",
    "\n",
    "end_clean = time.time()\n",
    "\n",
    "print(\"Time till cleaning corpus:\", end_clean - start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aww',\n",
       " 'match',\n",
       " 'background',\n",
       " 'colour',\n",
       " 'seemingly',\n",
       " 'stick',\n",
       " 'thank',\n",
       " 'talk',\n",
       " 'january_utc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 322843 number of words in the final dictionary\n"
     ]
    }
   ],
   "source": [
    "# create the dictionary\n",
    "dictionary = Dictionary(all_text)\n",
    "print(\"There are\", len(dictionary), \"number of words in the final dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "Wordlist from the sentence: ['aww', 'match', 'background', 'colour', 'seemingly', 'stick', 'thank', 'talk', 'january_utc']\n"
     ]
    }
   ],
   "source": [
    "# convert into lookup tuples within the dictionary using doc2bow\n",
    "print(dictionary.doc2bow(all_text.iloc[1]))\n",
    "print(\"Wordlist from the sentence:\", all_text.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordlist from the dictionary lookup: aww background colour january_utc match seemingly stick talk thank\n"
     ]
    }
   ],
   "source": [
    "# to check\n",
    "print(\"Wordlist from the dictionary lookup:\",\n",
    "      dictionary[21],\n",
    "      dictionary[22],\n",
    "      dictionary[23],\n",
    "      dictionary[24],\n",
    "      dictionary[25],\n",
    "      dictionary[26],\n",
    "      dictionary[27],\n",
    "      dictionary[28],\n",
    "      dictionary[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till corpus creation: 504.83364510536194 s\n"
     ]
    }
   ],
   "source": [
    "# scale it to all text\n",
    "corpus = [dictionary.doc2bow(text) for text in all_text]\n",
    "end_corpus = time.time()\n",
    "print(\"Time till corpus creation:\", end_corpus - start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till LDA model creation: 879.0080683231354 s\n"
     ]
    }
   ],
   "source": [
    "# create the LDA model\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=15, id2word=dictionary)\n",
    "\n",
    "end_lda = time.time()\n",
    "print(\"Time till LDA model creation:\", end_lda - start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_viz = time.time()\n",
    "print(\"Time till viz:\", end_viz - start_time, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chart Desc:**\n",
    "\n",
    "The above visuals are from the awesome pyLDAviz package which is the python version of R package LDAviz.\n",
    "\n",
    "The Left side shows the multi-dimensional \"word-space\" superimposed on two \"Principal components\" and the relative positions of all the topics.\n",
    "\n",
    "The size of the circle represents what % of the corpus it contains.\n",
    "\n",
    "The right side shows the word frequencies within the topic and in the whole corpus.\n",
    "\n",
    "Clearly, some of the topics show a pattern of toxicity (ie) have a high contribution from toxic words.\n",
    "\n",
    "Now let's feed these topics into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the topic probability matrix\n",
    "topic_probability_mat = ldamodel[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it to test and train\n",
    "train_matrix = topic_probability_mat[:train.shape[0]]\n",
    "test_matrix = topic_probability_mat[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(topic_probability_mat)\n",
    "del(corpus)\n",
    "del(all_text)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sparse format (Csr matrix)\n",
    "train_sparse = gensim.matutils.corpus2csc(train_matrix)\n",
    "test_sparse = gensim.matutils.corpus2csc(test_matrix)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"total time till Sparse mat creation\", end_time - start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom NB model\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "        \n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "        \n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, \n",
    "                                       dual=self.dual, \n",
    "                                       n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self\n",
    "    \n",
    "model = NbSvmClassifier(C=2, dual=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target columns\n",
    "target_x = train_sparse.transpose()\n",
    "TARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "target_y=train[TARGET_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_sparse)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, \n",
    "                                                      test_size=0.33, random_state=2018)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "preds_train = np.zeros((X_train.shape[0], y_train.shape[1]))\n",
    "preds_valid = np.zeros((X_valid.shape[0], y_valid.shape[1]))\n",
    "\n",
    "for i, j in enumerate(TARGET_COLS):\n",
    "    \n",
    "    print(\"Class:= \" + j)\n",
    "    \n",
    "    model.fit(X_train, y_train[j])\n",
    "    \n",
    "    preds_valid[:,i] = model.predict_proba(X_valid)[:, 1]\n",
    "    preds_valid[:,i] = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    train_loss_class = log_loss(y_train[j], preds_train[:,i])\n",
    "    valid_loss_class = log_loss(y_valid[j], preds_valid[:,i])\n",
    "    \n",
    "    print(\"Trainloss=log loss:\", train_loss_class)\n",
    "    print(\"Validloss=log loss:\", valid_loss_class)\n",
    "    \n",
    "    train_loss.append(train_loss_class)\n",
    "    valid_loss.append(valid_loss_class)\n",
    "    \n",
    "print(\"mean column-wise log loss:Train dataset\", np.mean(train_loss))\n",
    "print(\"mean column-wise log loss:Validation dataset\", np.mean(valid_loss))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"total time till NB base model creation\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credits\n",
    "#pyLDAviz\n",
    "#https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "\n",
    "#to be continued \n",
    "#to do next\n",
    "#paragraph vectors\n",
    "#https://arxiv.org/abs/1507.07998\n",
    "#https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

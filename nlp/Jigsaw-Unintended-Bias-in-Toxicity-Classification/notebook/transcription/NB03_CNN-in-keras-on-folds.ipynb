{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN in keras on folds\n",
    "\n",
    "[https://www.kaggle.com/artgor/cnn-in-keras-on-folds](https://www.kaggle.com/artgor/cnn-in-keras-on-folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1682203617576844873, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3157432729\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10356021244852916418\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "\n",
    "This is a basic kernel with CNN. In this kernel I train a CNN model on folds and calculate the competition metric (not simple auc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenet\n",
    "\n",
    "1. [Loading and processing data](#1)\n",
    "2. [Validation functions](#2)\n",
    "3. [Model](#3)\n",
    "4. [Training function](#4)\n",
    "5. [Train and predict](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-crawl-300d-2m', 'glove840b300dtxt', 'identity_individual_annotations.csv', 'sample_submission.csv', 'test.csv', 'test_private_expanded.csv', 'test_public_expanded.csv', 'toxicity_individual_annotations.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "\n",
    "import time\n",
    "import os\n",
    "print(os.listdir('../../input'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "I'll load preprocessed data from my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../input/train.csv')\n",
    "test = pd.read_csv('../../input/test.csv')\n",
    "\n",
    "# after processinig some of the texts are emply\n",
    "train['comment_text'] = train['comment_text'].fillna('')\n",
    "test['comment_text'] = test['comment_text'].fillna('')\n",
    "\n",
    "sub = pd.read_csv('../../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = list(train['comment_text'].values) + list(test['comment_text'].values)\n",
    "max_features = 300000\n",
    "tk = Tokenizer(lower=True, filters='', num_words=max_features)\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path1 = \"../../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "embedding_path2 = \"../../input/glove840b300dtxt/glove.840B.300d.txt\"\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr) :\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def build_matrix(embedding_path, tokenizer):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path,encoding='utf-8'))\n",
    "    \n",
    "    word_index = tk.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        \n",
    "        if i >= max_features:\n",
    "            continue\n",
    "            \n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "# combining embeddings from this kernel: \n",
    "# https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution\n",
    "embedding_matrix = np.concatenate([build_matrix(embedding_path1, tk),\n",
    "                                   build_matrix(embedding_path2, tk)],\n",
    "                                  axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(train['target'] >= 0.5, True, False) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = ['male',\n",
    "                    'female',\n",
    "                    'homosexual_gay_or_lesbian',\n",
    "                    'christian',\n",
    "                    'jewish',\n",
    "                    'muslim',\n",
    "                    'black',\n",
    "                    'white',\n",
    "                    'psychiatric_or_mental_illness']\n",
    "\n",
    "for col in identity_columns + ['target']:\n",
    "    train[col] = np.where(train[col] >= 0.5, True, False)\n",
    "    \n",
    "n_fold = 5\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc' # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc' # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "def compute_subgroup_auc(df, subgroup, label, oof_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    \n",
    "    return compute_auc(subgroup_examples[label],\n",
    "                       subgroup_examples[oof_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"\n",
    "    Computes the AUC of the within-subgroup negative examples \n",
    "    and the background positive examples.\n",
    "    \"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    \n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"\n",
    "    Computes the AUC of the within-subgroup positive examples\n",
    "    and the background negative examples.\n",
    "    \"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negativie_examples)\n",
    "    \n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"\n",
    "    Computes per-subgroup metrics for all subgroups and one model.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        \n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        \n",
    "        records.append(record)\n",
    "        \n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "def calculate_overall_auc(df, oof_name):\n",
    "    true_labels = df['target']\n",
    "    predicted_labels = df[oof_name]\n",
    "    \n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "                             power_mean(bias_df[BPSN_AUC], POWER),\n",
    "                             power_mean(bias_df[BNSP_AUC], POWER)])\n",
    "    \n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding attention from this kernel: \n",
    "# https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, step_dim, \n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        \n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer = self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "        self.features_dim = input_shape[-1]\n",
    "        \n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "            \n",
    "        else:\n",
    "            self.b = None\n",
    "            \n",
    "        self.built = True\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                              K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "        \n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "            \n",
    "        eij = K.tanh(eij)\n",
    "        \n",
    "        a = K.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "            \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return K.sum(weighted_input, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "def build_model(X_train, y_train, X_valid, y_valid,\n",
    "                max_len, max_features, embed_size, embedding_matrix,\n",
    "                lr=0.0, lr_d=0.0, spatial_dr=0.0,\n",
    "                dense_units=128, conv_size=128,\n",
    "                dr=0.2, patience=3, fold_id=1):\n",
    "\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,\n",
    "                                  save_best_only=True, mode='min')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=patience)\n",
    "\n",
    "    inp = Input(shape=(max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size * 2, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "    att = Attention(max_len)(x1)\n",
    "\n",
    "    # from benchmark kernel\n",
    "    x = Conv1D(conv_size, 2, activation='relu', padding='same')(x1)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Conv1D(conv_size, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = concatenate([x, att])\n",
    "\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu')(x))\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr, decay=lr_d), metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_valid, y_valid),\n",
    "              verbose=2, callbacks=[early_stop, check_point])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, tokenizer, max_len):\n",
    "    \n",
    "    oof = np.zeros((len(X), 1))\n",
    "    prediction = np.zeros((len(X_test), 1))\n",
    "    scores = []\n",
    "    \n",
    "    test_tokenized = tokenizer.texts_to_sequences(test['comment_text'])\n",
    "    X_test = pad_sequences(test_tokenized, maxlen=max_len)\n",
    "    \n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        \n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        valid_df = X_valid.copy()\n",
    "        \n",
    "        train_tokenized = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "        valid_tokenized = tokenizer.texts_to_sequences(X_valid['comment_text'])\n",
    "        \n",
    "        X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "        X_valid = pad_sequences(valid_tokenized, maxlen=max_len)\n",
    "        \n",
    "        model = build_model(X_train, y_train, X_valid, y_valid,\n",
    "                            max_len, max_features, embed_size, embedding_matrix,\n",
    "                            lr=1e-3, lr_d=0, spatial_dr=0.1, \n",
    "                            dense_units=128, conv_size=128,\n",
    "                            dr=0.1, patience=3, fold_id=fold_n)\n",
    "        \n",
    "        pred_valid = model.predict(X_valid)\n",
    "        oof[valid_index] = pred_valid\n",
    "        valid_df[oof_name] = pred_valid\n",
    "        \n",
    "        bias_metrics_df = compute_bias_metrics_for_model(valid_df,\n",
    "                                                         identity_columns,\n",
    "                                                         oof_name,\n",
    "                                                         'target')\n",
    "        scores.append(get_final_metric(bias_metrics_df,\n",
    "                                       calculate_overall_auc(valid_df, oof_name)))\n",
    "        \n",
    "        prediction += model.predict(X_test, batch_size=1024, verbose=1)\n",
    "        \n",
    "        prediction /= n_fold\n",
    "        \n",
    "        # print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "        \n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Nov 28 23:53:26 2019\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\AppData\\Local\\conda\\conda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\AppData\\Local\\conda\\conda\\envs\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\AppData\\Local\\conda\\conda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/3\n",
      " - 602s - loss: 0.1655 - acc: 0.9414 - val_loss: 0.1558 - val_acc: 0.9441\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15578, saving model to best_model_fold_0.hdf5\n",
      "Epoch 2/3\n",
      " - 695s - loss: 0.1547 - acc: 0.9447 - val_loss: 0.1548 - val_acc: 0.9443\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15578 to 0.15482, saving model to best_model_fold_0.hdf5\n",
      "Epoch 3/3\n",
      " - 638s - loss: 0.1508 - acc: 0.9457 - val_loss: 0.1526 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15482 to 0.15260, saving model to best_model_fold_0.hdf5\n"
     ]
    }
   ],
   "source": [
    "oof_name = 'predicted_target'\n",
    "max_len = 250\n",
    "\n",
    "oof, prediction, scores = train_model(X=train, X_test=test, y=train['target'],\n",
    "                                      tokenizer=tk, max_len=max_len)\n",
    "\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}'.format(np.mean(scores), np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
